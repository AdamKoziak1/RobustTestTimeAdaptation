Start a tmux session:
tmux new -s tta

Run your long training or evaluation command.

Detach safely anytime:
Ctrl + B, then D

Reconnect:
tmux attach -t tta

TRAIN: python train.py --output train_output --dataset PACS --data_file /home/adam/Downloads/RobustTestTimeAdaptation

**** CUDA_VISIBLE_DEVICES=1 python train.py --output train_output --dataset DomainNet --test_envs 4 --batch_size 64



ADAPT: python unsupervise_adapt.py --dataset PACS --data_dir /home/adam/Downloads/RobustTestTimeAdaptation/datasets/PACS --adapt_alg TENT --batch_size 1 --attack_rate 50 --mask_id 0
./run_domain.sh 

resnet18 first, then resnet50. insufficient VRAM for VIT (at default batch size at least)


TODO:
implement lambda1 loss
implement lambda2 loss
  make L_0 (stop gradient) a hyperparameter


finish train on officehome 3
 (hold off on domainnet 4,5)
run data_generation on officenet

generate masks on vlcs, 
unsupervise_adapt on all dataset/mask/alg combs (compare against 5+ methods, ideally SOTA)

add stddev in results

look at t3a, why is it robust

ablation on loss terms